# -*- coding: utf-8 -*-
"""Bayseian Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iyKONpRygjAGa-4IitjV1IFj28drumX6
"""

import numpy as np
from scipy.stats import norm

# Simulated data
np.random.seed(42)
spam_data = np.random.normal(loc=[20, 0.1], scale=[5, 0.05], size=(500, 2))
non_spam_data = np.random.normal(loc=[10, 0.02], scale=[3, 0.02], size=(500, 2))

# Combine spam and non-spam data
data = np.vstack([spam_data, non_spam_data])
labels = np.concatenate([np.ones(500), np.zeros(500)])

# Bayesian approach
def bayesian_classifier(word_count, free_frequency):
    # Prior knowledge (assuming 50% chance of spam)
    prior_spam = 0.5
    prior_not_spam = 1 - prior_spam

    # Likelihood (assuming normal distribution)
    likelihood_spam = norm.pdf(word_count, loc=20, scale=5) * norm.pdf(free_frequency, loc=0.1, scale=0.05)
    likelihood_not_spam = norm.pdf(word_count, loc=10, scale=3) * norm.pdf(free_frequency, loc=0.02, scale=0.02)

    # Posterior inference using Bayes' theorem
    posterior_spam = (likelihood_spam * prior_spam) / ((likelihood_spam * prior_spam) + (likelihood_not_spam * prior_not_spam))
    posterior_not_spam = 1 - posterior_spam

    return posterior_spam, posterior_not_spam

# Make predictions for a new email
new_email_word_count = 15
new_email_free_frequency = 0.05
spam_probability, not_spam_probability = bayesian_classifier(new_email_word_count, new_email_free_frequency)

# Print predictions
print(f"Probability of Spam: {spam_probability}")
print(f"Probability of Not Spam: {not_spam_probability}")
print("Prediction: Spam" if spam_probability > not_spam_probability else "Prediction: Not Spam")